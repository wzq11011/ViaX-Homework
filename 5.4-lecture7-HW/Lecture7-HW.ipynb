{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms as tfs\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tf = tfs.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = ImageFolder('./Fruit-Images-Dataset/Training', transform = data_tf)\n",
    "test_set = ImageFolder('./Fruit-Images-Dataset/Test', transform = data_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = DataLoader(train_set, batch_size = 64, shuffle = True)\n",
    "test_data = DataLoader(test_set, batch_size = 128, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_net1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(conv_net1,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3)\n",
    "        self.batch_norm1 = nn.BatchNorm2d(32)\n",
    "        self.max_pool1 = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding = 1)\n",
    "        self.batch_norm2 = nn.BatchNorm2d(64)\n",
    "        self.max_pool2 = nn.MaxPool2d(2 ,2)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding = 1)\n",
    "        self.batch_norm3 = nn.BatchNorm2d(128)\n",
    "        self.max_pool3 = nn.MaxPool2d(2, 2)\n",
    "        self.conv4 = nn.Conv2d(128, 256, 3)\n",
    "        self.batch_norm4 = nn.BatchNorm2d(256)\n",
    "        self.max_pool4 = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(6400, 3200)\n",
    "        self.fc2 = nn.Linear(3200, 1600)\n",
    "        self.fc3 = nn.Linear(1600, 101)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.max_pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.max_pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.max_pool3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.batch_norm4(x)\n",
    "        x = self.max_pool4(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = conv_net1()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr = 0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_learning_rate(optimizer, lr):\n",
    "    for param_groups in optimizer.param_groups:\n",
    "        param_groups['lr'] = lr\n",
    "        \n",
    "def get_acc(output, label):\n",
    "    total = output.shape[0]\n",
    "    _, pred_label = output.max(1)\n",
    "    num_correct = (pred_label == label).sum().data.item()\n",
    "    return num_correct / total\n",
    "\n",
    "def train(net, train_data, test_data, epoch, optimizer, criterion):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    if torch.cuda.is_available():\n",
    "        net = net.cuda()\n",
    "    prev_time = datetime.now()\n",
    "    for epoch in range(epoch):\n",
    "        if epoch == 15:\n",
    "            set_learning_rate(optimizer, 0.001)\n",
    "        train_loss = 0\n",
    "        train_acc = 0\n",
    "        net = net.train()\n",
    "        for im, labels in train_data:\n",
    "            if torch.cuda.is_available():\n",
    "                im = im.cuda()\n",
    "                labels = labels.cuda()\n",
    "            \n",
    "            output = net(im)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.data.item()\n",
    "            train_acc += get_acc(output, labels)\n",
    "        cur_time = datetime.now()\n",
    "        h,remainder = divmod((cur_time - prev_time).seconds, 3600)\n",
    "        m,s = divmod(remainder, 60)\n",
    "        time_str = 'Time:%02d:%02d:%02d'%(h, m, s)\n",
    "        \n",
    "        test_loss = 0\n",
    "        test_acc = 0\n",
    "        net = net.eval()\n",
    "        for im, labels in test_data:\n",
    "            if torch.cuda.is_available():\n",
    "                im = im.cuda()\n",
    "                labels = labels.cuda()\n",
    "            output = net(im)\n",
    "            loss = criterion(output, labels)\n",
    "            test_loss += loss.data.item()\n",
    "            test_acc += get_acc(output, labels)\n",
    "        epoch_str = (\n",
    "                \"Epoch %d. Train Loss: %f, Train Acc: %f, Valid Loss: %f, Valid Acc: %f, \"\n",
    "                % (epoch, train_loss / len(train_data),\n",
    "                   train_acc / len(train_data), test_loss / len(test_data),\n",
    "                   test_acc / len(test_data)))\n",
    "        prev_time = cur_time\n",
    "        train_losses.append(train_loss / len(train_data))\n",
    "        test_losses.append(test_loss / len(test_data))\n",
    "        print(epoch_str + time_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Train Loss: 2.566145, Train Acc: 0.678380, Valid Loss: 0.807374, Valid Acc: 0.820117, Time:00:01:00\n",
      "Epoch 1. Train Loss: 0.463633, Train Acc: 0.879126, Valid Loss: 0.785649, Valid Acc: 0.867754, Time:00:01:13\n",
      "Epoch 2. Train Loss: 0.351352, Train Acc: 0.914989, Valid Loss: 0.482884, Valid Acc: 0.903589, Time:00:01:13\n",
      "Epoch 3. Train Loss: 0.215261, Train Acc: 0.945516, Valid Loss: 0.417356, Valid Acc: 0.921422, Time:00:01:13\n",
      "Epoch 4. Train Loss: 0.149103, Train Acc: 0.959777, Valid Loss: 0.950699, Valid Acc: 0.857563, Time:00:01:13\n",
      "Epoch 5. Train Loss: 0.130315, Train Acc: 0.965866, Valid Loss: 0.281978, Valid Acc: 0.939651, Time:00:01:13\n",
      "Epoch 6. Train Loss: 0.085828, Train Acc: 0.976945, Valid Loss: 0.389917, Valid Acc: 0.932518, Time:00:01:13\n",
      "Epoch 7. Train Loss: 0.389600, Train Acc: 0.922172, Valid Loss: 0.792895, Valid Acc: 0.902174, Time:00:01:13\n",
      "Epoch 8. Train Loss: 0.162028, Train Acc: 0.962342, Valid Loss: 0.756856, Valid Acc: 0.877264, Time:00:01:13\n",
      "Epoch 9. Train Loss: 0.167532, Train Acc: 0.964311, Valid Loss: 1.212630, Valid Acc: 0.890625, Time:00:01:14\n",
      "Epoch 10. Train Loss: 0.336869, Train Acc: 0.942940, Valid Loss: 0.649146, Valid Acc: 0.916723, Time:00:01:14\n",
      "Epoch 11. Train Loss: 0.192773, Train Acc: 0.963884, Valid Loss: 0.706127, Valid Acc: 0.940557, Time:00:01:13\n",
      "Epoch 12. Train Loss: 0.096174, Train Acc: 0.982043, Valid Loss: 2.026590, Valid Acc: 0.836277, Time:00:01:13\n",
      "Epoch 13. Train Loss: 0.588718, Train Acc: 0.923714, Valid Loss: 0.667367, Valid Acc: 0.930707, Time:00:01:14\n",
      "Epoch 14. Train Loss: 0.101629, Train Acc: 0.980631, Valid Loss: 1.541623, Valid Acc: 0.907212, Time:00:01:13\n",
      "Epoch 15. Train Loss: 0.024545, Train Acc: 0.994912, Valid Loss: 0.489835, Valid Acc: 0.953295, Time:00:01:13\n",
      "Epoch 16. Train Loss: 0.005996, Train Acc: 0.998548, Valid Loss: 0.480935, Valid Acc: 0.958390, Time:00:01:14\n",
      "Epoch 17. Train Loss: 0.005728, Train Acc: 0.998606, Valid Loss: 0.480142, Valid Acc: 0.959579, Time:00:01:14\n",
      "Epoch 18. Train Loss: 0.003637, Train Acc: 0.999255, Valid Loss: 0.453727, Valid Acc: 0.963032, Time:00:01:14\n",
      "Epoch 19. Train Loss: 0.005675, Train Acc: 0.999107, Valid Loss: 0.463309, Valid Acc: 0.961787, Time:00:01:14\n",
      "Epoch 20. Train Loss: 0.003175, Train Acc: 0.999179, Valid Loss: 0.390979, Valid Acc: 0.961900, Time:00:01:13\n",
      "Epoch 21. Train Loss: 0.003615, Train Acc: 0.999026, Valid Loss: 0.418765, Valid Acc: 0.962070, Time:00:01:13\n",
      "Epoch 22. Train Loss: 0.004854, Train Acc: 0.998895, Valid Loss: 0.396529, Valid Acc: 0.964561, Time:00:01:14\n",
      "Epoch 23. Train Loss: 0.006722, Train Acc: 0.998376, Valid Loss: 0.445286, Valid Acc: 0.957824, Time:00:01:14\n",
      "Epoch 24. Train Loss: 0.005426, Train Acc: 0.998816, Valid Loss: 0.499664, Valid Acc: 0.959069, Time:00:01:14\n"
     ]
    }
   ],
   "source": [
    "train(net, train_data, test_data, 25, optimizer, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
